### 线性回归
#### 1.多元线性回归
系数，标准误，t检验与p值
R^2 adjust R^2 F检验，p值

$$y = \alpha_1x_1 + \alpha_2x_2 + \cdots + \alpha_nx_n$$



#### 2.正则化（Lasso的L1惩罚，Ridge的L2惩罚）
核心目的是为了防止过拟合。但是超高的系数本身在现实意义上难以解释，比如X^3意味着有2个拐点，
现实中为什么会有这两个拐点呢，理论基础或者现实依据在哪呢。

多元中（变量过多的情况下）


与PCA和opls-da的区别


#### 3.Logistic Regression
所以logistic回归就是在用线性回归的预测结果去逼近真实标记的对数几率
几率：已知事件发生的概率为P，不发生的概率为1-P，则几率为P/1-P

$$ sigmoid : Y = \frac {1}{1+e^{-x}}  $$
x取值是负无穷到正无穷，映射到y上为（0，1），中间为0.5
$$y = \alpha_1x_1 + \alpha_2x_2 + \cdots + \alpha_nx_n$$
y就是一个值，映射一下，



#### 手写
#### sklearn
#### pytorch
前馈
反馈
更新