## 线性回归
### 1.线性回归
1. 用来做预测并没什么用
2. 在经济学中对系数的解释更重要（p值小于0.05）
3. 系数的统计显著性固然重要，但经济显著性可能更重要
4. 方程取log的解释
5. 核心因变量是虚拟变量的解释
6. 在因果推断下的解释
   
#### 简单线性回归
$$\hat{y_i} = w x_i + b$$

原理：使得预测值与真实值的距离之和最小，由于距离有正有负，故而取绝对值：$\sum_{i=1}^n \lvert y_i - \hat{y_i}  \rvert$ ，不过绝对值不利于求导，故而取平方$\sum_{i=1}^n(y_i - \hat{y_i})^2$

优化目的：均方误差最小
$$(w^*,b^*) = E(w,b) = \mathop{\arg\min}\limits_{(w,b)} \sum_{i=1}^n(y_i - \hat{y_i})^2$$
对$w$和$b$求导
$$
$$
#### 多元线性回归




系数，标准误，t检验与p值
R^2 adjust R^2 F检验，p值

$$y = \alpha_1x_1 + \alpha_2x_2 + \cdots + \alpha_nx_n$$


```r
library(tidyverse)
library(MASS)

model = lm(mpg ~ disp + wt ,data = mtcars) 
summary(model)

# 手写复现
# 数据准备
data <- data.frame(
  x1 = mtcars$disp,
  x2 = mtcars$wt,
  y = mtcars$mpg 
)
X<-  data.frame(
  x0 = rep(1,32), #不能忘了这个！！！
  x1 = mtcars$disp,
  x2 = mtcars$wt)%>%
  as.matrix()
Y <- data$y%>%
  as.matrix()

# 求解估计值（矩阵算法，t(X) %*% X必须可以求逆）
inv_Q <- solve(t(X) %*% X)
ginv(t(X) %*% X) # library(MASS)
est <- inv_Q %*% t(X) %*% Y

# 同方差假设下的估计值标注误homo_se
residuals <- (Y - X %*% est) %>%
    as.vector() # 残差
sigma2 <- (residuals - mean(residuals))^2 %>% 
    sum() # 残差平方和
sigma2 <- sigma2/(length(residuals) -3 ) # 除以自由度
homo_se <- (sigma2 * inv_Q) %>% 
    diag() %>%  # 取迹
    sqrt()  

# t值 系数/标准误
# p值 双尾：2 * pt(abs(t值), df = 自由度, lower.tail = FALSE)

# r^2在经济学中并不重要
# 

    
```


#### 2.正则化（Lasso的L1惩罚，Ridge的L2惩罚）
核心目的是为了防止过拟合。但是超高的系数本身在现实意义上难以解释，比如X^3意味着有2个拐点，
现实中为什么会有这两个拐点呢，理论基础或者现实依据在哪呢。

多元中（变量过多的情况下）


与PCA和opls-da的区别


#### 3.Logistic Regression
所以logistic回归就是在用线性回归的预测结果去逼近真实标记的对数几率
几率：已知事件发生的概率为P，不发生的概率为1-P，则几率为P/1-P

$$ sigmoid : Y = \frac {1}{1+e^{-x}}  $$
x取值是负无穷到正无穷，映射到y上为（0，1），中间为0.5
$$y = \alpha_1x_1 + \alpha_2x_2 + \cdots + \alpha_nx_n$$
y就是一个值，映射一下，



#### 手写
#### sklearn
#### pytorch
前馈
反馈
更新